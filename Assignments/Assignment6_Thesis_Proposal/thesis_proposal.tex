\documentclass[a4paper, 12pt]{article}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}

\author{Inge Becht}
\title{Assignment 6 - Project Proposal}

\begin{document}
\maketitle

\section{Introduction}
On of the challenges that is studies is creating artificial agents that can make
decisions based on a game situation, without the human players having the
feeling that the AI is cheating. Creating such a fair artificial intelligence can be achieved by
giving it only partial knowledge about the game world, for example by keeping parts of
the game map unknown or by not knowing the position of the opponents at all
times. In my thesis I want to focus on the latter by seeing if it is possible to
\emph{succesfully} build a motion model for a game of Capture the Flag that
predicts enemy position and by checking if this \emph{improve} the performance of an
existing game AI. It is clear that this problem has multiple aspects to it. Not
only is there a need for an implementation of the motion model itself, there is
also a need for building a reasoner that takes actions on the prediction of the
motion model. In the implementation section both sides of the research will be
more thoroughly explained. 

%\emph{Can the Maximum Entropy Inverse Reinforcement Learning Algorithm be
%adapted for opponent motion modelling in a game of Capture the Flag?}
%\emph{Can Maximum Entropy Inverse Reinforcement Learning be applied for
%opponent modeling to create a better AI in ca game of Capture the Flag?}
%\emph{Can I improve}

\section{Background Information}

\subsection{Capture the Flag}
Capture The Flag (CTF) is a well known combat objective found in video games
like the Call of Duty series and Counter strike, in which two enemy teams try to
catch the other one's flag from its spawn area and bringing it back to base.
Both teams can shoot at each other and in case the flag bearer gets killed, the
flag will stay at its posiiton until its team retrieves it or another enemy
opponent catches it and becomes the new flag bearer. The game is won after one
of the teams has catched the flag for a pre-determined number of times.


\section{Related Works}
There have been different studies on the topic of opponent motion modelling.
In the works of \citep{Hladky_anevaluation} two different models for opponent
position prediction are tested in the team oriented game Counter strike. 
The tested motion models were
Hidden Semi Markov Models and Particle filters.
The research mostly thus concentrated on researching how human-like the
models are, but does not research how to succesfully integrate such a prediction system in a multi-agent AI system.
In \citep{weber2011aiide} a particle filter is used, but
the research is more directed towards how much does
integrating this in an AI system can enhance performance. Essentially it answers
the same question as brought up in this thesis proposal, but on a different game
domain (Real Time Strategy instead of First Person Shooter) as well was using a
different technique of motion modelling. The game Star Craft was used and the EISBot was enhanced with their developed reasoning capabilities.
Instead of trying to predict the opponent position, the authors of
\citep{Laird:2001:KYG:375735.376343} focused on creating a reasoner that
anticipates behavior of an opponent in Quake. They did this by enhancing the
Soar Quakebot with anticipation strategies. This approach can be of use towards
creating a reasoner, but differs in it not being used on a motion model, as well
as not answering the question \emph{can it succesfully improve AI?}
In the paper \citep{6374144} both the combination of making predictive models
for opponent positions is discusses as well as a way of intercepting the
opponents in game. The predictive models are made using a particle filter both
with Maximum Enthorpy Inverse Reinforcement Learning and Brownian motion to test
which works best. Although this research is similar to my own, it
is different in multiple aspects. A game with different goals is used to test
their motion model and interception techniques. There is only one agent against
multiple different opponents with the only taks of intercepting the opponent. I
will be using the technique of ME IRL, and will talk more about the way this is
intergrated in the next section.

\section{Method and Approach}

It is clear that this problem has multiple aspects to it. Not
only is there a need for an implementation of the motion model itself, there is
also a need for building a reasoner that takes actions on the prediction of the
motion model.
-Want to use Inverse reinfocement learning for this purpose
The game environment will be used is called the AISandbox, a tool developed my
makes of the AIGameDev blog and Guerilla Games.

It would not be efficient to work from scratch, so code was used from the
AISandbox Challende to implement this in. The code is written in Java. The way
the code works is by 
-Use of code Terminator

\section{Evaluation}
There are multiple ways to evaluate the model. As we have seen there have been
instances of research that checks if a model makes humanlike predictions % cite
or if its prediction has the smallest possible errors %cite
The way the implementation of the research problem will be evaluated depends on
how far the implementation will be finished.

\section{plan}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
